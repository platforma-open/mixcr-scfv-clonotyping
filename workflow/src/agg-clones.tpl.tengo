ll := import("@platforma-sdk/workflow-tengo:ll")
self := import("@platforma-sdk/workflow-tengo:tpl")
pConstants := import("@platforma-sdk/workflow-tengo:pframes.constants")
units := import("@platforma-sdk/workflow-tengo:units")
pt := import("@platforma-sdk/workflow-tengo:pt")
math := import("math")
json := import("json")

self.defineOutputs("tsv")

self.body(func(inputs) {
	inputData := inputs[pConstants.VALUE_FIELD_NAME]
	inputDataMeta := inputData.getDataAsJson()
	inputMap := inputData.inputs()
	numberOfSamples := len(inputMap)

	ll.assert(inputDataMeta.keyLength == 1, "unexpected number of aggregation axes")

	mainAbundanceColumnNormalized := inputs.mainAbundanceColumnNormalized
	mainAbundanceColumnUnnormalized := inputs.mainAbundanceColumnUnnormalized
	cloneColumns := inputs.cloneColumns
	cloneColumnSpecs := inputs.cloneColumnSpecs

	addedColumns := {}
	baseSchemaForRead := []
	addSchema := func(column, t) {
		if !addedColumns[column] {
			baseSchemaForRead += [ { column: column, type: t } ]
			addedColumns[column] = true
		}
	}

	for colSpec in cloneColumnSpecs {
		addSchema(colSpec.column, "String")
	}
	addSchema(mainAbundanceColumnNormalized, "String")
	addSchema(mainAbundanceColumnUnnormalized, "String")
	addSchema("clonotypeKey", "String")

	wf := pt.workflow().
		inMediumQueue().
		mem(int(math.max(numberOfSamples, 64)) * units.GiB).
		cpu(int(math.max(numberOfSamples, 32)))

	dataFrames := []
	for sKey, inputFile in inputMap {
		key := json.decode(sKey)
		if len(key) != 1 {
			ll.panic("malformed key: %v", sKey)
		}
		sampleId := key[0]
		df := wf.frame({
			file: inputFile,
			xsvType: "tsv",
			schema: baseSchemaForRead
		}, {
			id: "table_" + sampleId,
			inferSchema: false
		})
		dataFrames = append(dataFrames, df)
	}

	if len(dataFrames) == 0 {
		ll.panic("no input files found")
	}

	currentDf := dataFrames[0]
	if len(dataFrames) > 1 {
		currentDf = pt.concat(dataFrames)
	}

	numericTypes := {
		"Int": "Int",
		"Long": "Long",
		"Double": "Double"
	}
	numericColumns := {}
	for colSpec in cloneColumnSpecs {
		castType := numericTypes[colSpec.spec.valueType]
		if !is_undefined(castType) {
			numericColumns[colSpec.column] = castType
		}
	}
	numericColumns[mainAbundanceColumnNormalized] = "Double"
	numericColumns[mainAbundanceColumnUnnormalized] = "Long"

	castExprs := []
	for colName, castType in numericColumns {
		castExprs = append(castExprs,
			pt.when(pt.col(colName).eq("region_not_covered")).
				then(pt.lit(undefined)).
				otherwise(pt.col(colName)).
				cast(castType).
				alias(colName)
		)
	}
	currentDf = currentDf.withColumns(castExprs...)

	aggExpressions := []
	for col in cloneColumns {
		aggExpressions = append(aggExpressions,
			pt.col(col).maxBy(pt.col(mainAbundanceColumnNormalized)).alias(col)
		)
	}
	aggExpressions = append(aggExpressions,
		pt.col(mainAbundanceColumnNormalized).count().alias("sampleCount"),
		pt.col(mainAbundanceColumnUnnormalized).sum().alias(mainAbundanceColumnUnnormalized + "Sum")
	)

	aggregatedDf := currentDf.groupBy("clonotypeKey").agg(aggExpressions...)
	aggregatedDf.save("output.tsv")

	ptResult := wf.run()

	processedTsv := ptResult.getFile("output.tsv")

	return {
		tsv: processedTsv
	}
})

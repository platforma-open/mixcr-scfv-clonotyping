self := import("@platforma-sdk/workflow-tengo:tpl")
smart := import("@platforma-sdk/workflow-tengo:smart")
ll := import("@platforma-sdk/workflow-tengo:ll")
exec := import("@platforma-sdk/workflow-tengo:exec")
assets := import("@platforma-sdk/workflow-tengo:assets")
pcolumn := import("@platforma-sdk/workflow-tengo:pframes.pcolumn")
times := import("times")
text := import("text")
pframes := import("@platforma-sdk/workflow-tengo:pframes")
xsv := import("@platforma-sdk/workflow-tengo:pframes.xsv")
slices := import("@platforma-sdk/workflow-tengo:slices")
pConstants := import("@platforma-sdk/workflow-tengo:pframes.constants")
pt := import("@platforma-sdk/workflow-tengo:pt")
qcReportColumns := import(":qc-report-columns")

json := import("json")

self.defineOutputs("qcReportTable")

mixcrSw := assets.importSoftware("@platforma-open/milaboratories.software-mixcr:main")


self.body(func(inputs) {
    clnsDataHeavy := inputs.clnsDataHeavy
    clnsDataLight := inputs.clnsDataLight
    sampleIdAxisSpec := inputs.sampleIdAxisSpec
    chains := inputs.chains
    clonotypeTablesData := inputs.clonotypeTablesData
    hasUmi := inputs.hasUmi

    // Get all clna files from both chains
    clnaFilesHeavy := clnsDataHeavy.inputs()
    clnaFilesLight := clnsDataLight.inputs()
    
    // Generate raw TSV files for each chain separately
    heavyRawTsv := undefined
    if len(clnaFilesHeavy) > 0 {
        exportReportCmdHeavy := exec.builder().
            software(mixcrSw).
            env("MI_USE_SYSTEM_CA", "true").
            secret("MI_LICENSE", "MI_LICENSE").
            arg("exportReportsTable")
        
        for i, clnaFile in clnaFilesHeavy {
            fileName := json.decode(i)[0] + "_heavy.clna"
            exportReportCmdHeavy.addFile(fileName, clnaFile)
            exportReportCmdHeavy.arg(fileName)
        }
        
        exportReportCmdHeavy.arg("qc-report-heavy.tsv").saveFile("qc-report-heavy.tsv")
        if !is_undefined(inputs.heavyLibrary) {
            exportReportCmdHeavy.addFile("heavyLibrary.json", inputs.heavyLibrary)
        }
        resultHeavy := exportReportCmdHeavy.run()
        heavyRawTsv = resultHeavy.getFile("qc-report-heavy.tsv")
    }
    
    lightRawTsv := undefined
    if !inputs.suppressLight {
        exportReportCmdLight := exec.builder().
            software(mixcrSw).
            env("MI_USE_SYSTEM_CA", "true").
            secret("MI_LICENSE", "MI_LICENSE").
            arg("exportReportsTable")
        
        for i, clnaFile in clnaFilesLight {
            fileName := json.decode(i)[0] + "_light.clna"
            exportReportCmdLight.addFile(fileName, clnaFile)
            exportReportCmdLight.arg(fileName)
        }
        
        exportReportCmdLight.arg("qc-report-light.tsv").saveFile("qc-report-light.tsv")
        if !is_undefined(inputs.lightLibrary) {
            exportReportCmdLight.addFile("lightLibrary.json", inputs.lightLibrary)
        }
        resultLight := exportReportCmdLight.run()
        lightRawTsv = resultLight.getFile("qc-report-light.tsv")
    }
    
    // Use pt to process the TSV files
    wf := pt.workflow().
        inMediumQueue().
        mem("8GiB").
        cpu(2)
    
    // Process heavy chain separately
    processChain := func(rawTsvFile, suffix) {
        if is_undefined(rawTsvFile) {
            return undefined
        }
        
        // Load the raw TSV file as a DataFrame
        df := wf.frame(rawTsvFile, {
            xsvType: "tsv",
            inferSchema: false
        })
        
        // Extract sampleId from fileName (remove suffix)
        suffixLen := len(suffix)
        processedDf := df.withColumns(
            pt.col("fileName").strSlice(0, pt.col("fileName").strLenChars().minus(suffixLen)).alias("sampleId")
        )
        
        // Cast numeric columns before aggregation
        processedDfWithTypes := processedDf.withColumns(
            pt.col("totalReads").cast("Long").alias("totalReads"),
            pt.col("totalClonotypes").cast("Long").alias("totalClonotypes"),
            pt.col("readsUsedInClonotypes").cast("Long").alias("readsUsedInClonotypes"),
            pt.col("align.successAligned").cast("Long").alias("align.successAligned"),
            pt.col("align.alignmentsFailed").cast("Long").alias("align.alignmentsFailed")
        )
        
        // Group by sampleId and aggregate
        aggregatedDf := processedDfWithTypes.groupBy("sampleId").agg(
            pt.col("totalReads").sum().alias("totalReads"),
            pt.col("totalClonotypes").sum().alias("totalClonotypes"),
            pt.col("readsUsedInClonotypes").sum().alias("readsUsedInClonotypes"),
            pt.col("align.successAligned").sum().alias("align.successAligned"),
            pt.col("align.alignmentsFailed").sum().alias("align.alignmentsFailed"),
            pt.col("MiXCRVersion").first().alias("MiXCRVersion")
        )
        
        // Don't calculate percentages here - will be done after merging chains
        return aggregatedDf
    }
    
    // Process each chain separately
    heavyDf := processChain(heavyRawTsv, "_heavy.clna")
    lightDf := processChain(lightRawTsv, "_light.clna")
    
    // Save processed DataFrames for each chain
    if !is_undefined(heavyDf) {
        heavyDf.save("qc-report-processed-heavy.tsv", { xsvType: "tsv" })
    }
    if !is_undefined(lightDf) {
        lightDf.save("qc-report-processed-light.tsv", { xsvType: "tsv" })
    }
    
    // Join heavy and light DataFrames instead of concatenating
    mergedDf := undefined
    if !is_undefined(heavyDf) && !is_undefined(lightDf) {
        // Both chains exist - join on sampleId and sum values
        // Cast to Long first to ensure proper types
        heavySelected := heavyDf.withColumns(
            pt.col("totalReads").cast("Long").alias("totalReads"),
            pt.col("totalClonotypes").cast("Long").alias("totalClonotypes"),
            pt.col("readsUsedInClonotypes").cast("Long").alias("readsUsedInClonotypes"),
            pt.col("align.successAligned").cast("Long").alias("align.successAligned"),
            pt.col("align.alignmentsFailed").cast("Long").alias("align.alignmentsFailed")
        ).select(
            pt.col("sampleId"),
            pt.col("totalReads").alias("totalReads_heavy"),
            pt.col("totalClonotypes").alias("totalClonotypes_heavy"),
            pt.col("readsUsedInClonotypes").alias("readsUsedInClonotypes_heavy"),
            pt.col("align.successAligned").alias("align.successAligned_heavy"),
            pt.col("align.alignmentsFailed").alias("align.alignmentsFailed_heavy"),
            pt.col("MiXCRVersion").alias("MiXCRVersion_heavy")
        )
        lightSelected := lightDf.withColumns(
            pt.col("totalReads").cast("Long").alias("totalReads"),
            pt.col("totalClonotypes").cast("Long").alias("totalClonotypes"),
            pt.col("readsUsedInClonotypes").cast("Long").alias("readsUsedInClonotypes"),
            pt.col("align.successAligned").cast("Long").alias("align.successAligned"),
            pt.col("align.alignmentsFailed").cast("Long").alias("align.alignmentsFailed")
        ).select(
            pt.col("sampleId"),
            pt.col("totalReads").alias("totalReads_light"),
            pt.col("totalClonotypes").alias("totalClonotypes_light"),
            pt.col("readsUsedInClonotypes").alias("readsUsedInClonotypes_light"),
            pt.col("align.successAligned").alias("align.successAligned_light"),
            pt.col("align.alignmentsFailed").alias("align.alignmentsFailed_light"),
            pt.col("MiXCRVersion").alias("MiXCRVersion_light")
        )
        
        // Full outer join to include samples from both chains - keep all columns from both sides
        joined := heavySelected.join(lightSelected, { how: "full", on: ["sampleId"] })
        
        // Cast all numeric columns to Long and keep all columns (don't sum)
        mergedDf = joined.withColumns(
            pt.col("totalReads_heavy").fillNull(0).cast("Long").alias("totalReads_heavy"),
            pt.col("totalClonotypes_heavy").fillNull(0).cast("Long").alias("totalClonotypes_heavy"),
            pt.col("readsUsedInClonotypes_heavy").fillNull(0).cast("Long").alias("readsUsedInClonotypes_heavy"),
            pt.col("align.successAligned_heavy").fillNull(0).cast("Long").alias("align.successAligned_heavy"),
            pt.col("align.alignmentsFailed_heavy").fillNull(0).cast("Long").alias("align.alignmentsFailed_heavy"),
            pt.col("totalReads_light").fillNull(0).cast("Long").alias("totalReads_light"),
            pt.col("totalClonotypes_light").fillNull(0).cast("Long").alias("totalClonotypes_light"),
            pt.col("readsUsedInClonotypes_light").fillNull(0).cast("Long").alias("readsUsedInClonotypes_light"),
            pt.col("align.successAligned_light").fillNull(0).cast("Long").alias("align.successAligned_light"),
            pt.col("align.alignmentsFailed_light").fillNull(0).cast("Long").alias("align.alignmentsFailed_light")
        )
    } else if !is_undefined(heavyDf) {
        // Only heavy chain exists - use _heavy suffix for consistency
        mergedDf = heavyDf.withColumns(
            pt.col("totalReads").cast("Long").alias("totalReads"),
            pt.col("totalClonotypes").cast("Long").alias("totalClonotypes"),
            pt.col("readsUsedInClonotypes").cast("Long").alias("readsUsedInClonotypes"),
            pt.col("align.successAligned").cast("Long").alias("align.successAligned"),
            pt.col("align.alignmentsFailed").cast("Long").alias("align.alignmentsFailed")
        ).select(
            pt.col("sampleId"),
            pt.col("totalReads").alias("totalReads_heavy"),
            pt.col("totalClonotypes").alias("totalClonotypes_heavy"),
            pt.col("readsUsedInClonotypes").alias("readsUsedInClonotypes_heavy"),
            pt.col("align.successAligned").alias("align.successAligned_heavy"),
            pt.col("align.alignmentsFailed").alias("align.alignmentsFailed_heavy"),
            pt.col("MiXCRVersion").alias("MiXCRVersion_heavy")
        )
    } else if !is_undefined(lightDf) {
        // Only light chain exists - use _light suffix for consistency
        mergedDf = lightDf.withColumns(
            pt.col("totalReads").cast("Long").alias("totalReads"),
            pt.col("totalClonotypes").cast("Long").alias("totalClonotypes"),
            pt.col("readsUsedInClonotypes").cast("Long").alias("readsUsedInClonotypes"),
            pt.col("align.successAligned").cast("Long").alias("align.successAligned"),
            pt.col("align.alignmentsFailed").cast("Long").alias("align.alignmentsFailed")
        ).select(
            pt.col("sampleId"),
            pt.col("totalReads").alias("totalReads_light"),
            pt.col("totalClonotypes").alias("totalClonotypes_light"),
            pt.col("readsUsedInClonotypes").alias("readsUsedInClonotypes_light"),
            pt.col("align.successAligned").alias("align.successAligned_light"),
            pt.col("align.alignmentsFailed").alias("align.alignmentsFailed_light"),
            pt.col("MiXCRVersion").alias("MiXCRVersion_light")
        )
    } else {
        ll.panic("No chain data to process")
    }
    
    if is_undefined(mergedDf) {
        ll.panic("mergedDf is undefined")
    }
    
    aggregatedDf := mergedDf
    
    // Determine which chains actually have data
    chainsWithData := []
    if !is_undefined(heavyDf) {
        chainsWithData = append(chainsWithData, "IGHeavy")
    }
    if !is_undefined(lightDf) {
        chainsWithData = append(chainsWithData, "IGLight")
    }
    
    // Calculate percentages for each chain separately (only for chains with data)
    finalDf := aggregatedDf
    for chain in chainsWithData {
        suffix := undefined
        if chain == "IGHeavy" {
            suffix = "heavy"
        } else if chain == "IGLight" {
            suffix = "light"
        }
        if is_undefined(suffix) { continue }
        
        finalDf = finalDf.withColumns(
            (pt.col("align.successAligned_" + suffix).cast("Double").truediv(pt.col("totalReads_" + suffix).fillNull(1).cast("Double")).multiply(100)).alias("align.successAlignedPercents_" + suffix)
        )
    }

    // Calculate real number of exported (productive) clonotypes per sample from clonotype tables
    countDfs := []
    if !is_undefined(clonotypeTablesData) {
        for key, clonesFile in clonotypeTablesData.inputs() {
            sampleId := json.decode(key)[0]
            dfCountSource := wf.frame(clonesFile, { xsvType: "tsv", inferSchema: false, schema: [ { column: "readCount", type: "Double" } ] })
            dfCount := dfCountSource.select(
                pt.lit(sampleId).alias("sampleId"),
                pt.col("clonotypeKey").count().alias("exportedClonotypes"),
                pt.col("readCount").round().cast("Long").sum().alias("readsUsedInClonotypes")
            )
            countDfs = append(countDfs, dfCount)
        }
    }

    aggregatedCounts := undefined
    if len(countDfs) > 0 {
        countsDf := len(countDfs) > 1 ? pt.concat(countDfs) : countDfs[0]
        aggregatedCounts = countsDf.groupBy("sampleId").agg(
            pt.col("exportedClonotypes").sum().alias("exportedClonotypes"),
            pt.col("readsUsedInClonotypes").sum().alias("readsUsedInClonotypesNew")
        )
    }

    // Join counts and overwrite totalClonotypes
    joinedDf := finalDf
    if !is_undefined(aggregatedCounts) {
        joinedDf = finalDf.join(aggregatedCounts, { how: "left", on: ["sampleId"] })
    }

    // Finalize: cast/fill totals
    // Set totalClonotypes from exportedClonotypes (readsUsedInClonotypes columns remain separate for heavy/light)
    finalProcessedDf := joinedDf.withColumns(
        pt.col("exportedClonotypes").fillNull(0).cast("Long").alias("totalClonotypes")
    )
    
    // Save the final DataFrame back to TSV
    finalProcessedDf.save("qc-report-processed.tsv", {
        xsvType: "tsv"
    })
    
    // Run the pt workflow
    wfResult := wf.run()
    
    tsvFile := wfResult.getFile("qc-report-processed.tsv")

    // Only generate column spec for chains that actually have data
    qcReportColumnsResult := qcReportColumns(hasUmi, sampleIdAxisSpec, chainsWithData)
    reportColumnsSpec := qcReportColumnsResult.reportColumnsSpec

    qcReportTable := xsv.importFile(
		tsvFile,
		"tsv",
		reportColumnsSpec,
		{ cpu: 1, mem: "16GiB" }
	)

    
    return {
        qcReportTable: qcReportTable
    }
})


